# -*- coding: utf-8 -*-

from os import name
import requests
import fake_useragent
from bs4 import BeautifulSoup
import re
from fuzzywuzzy import process
import asyncio
import aiohttp
import json
import lxml

def parser_1(text, tag):
    b = '' 
    pagen = []
    ssulka_na_resume = []
    url_pars = []
    vakans = text
    gorod = tag
    numbe  = ['113', '1', '2', '2', '2019', '16', '5', '40', '1438', '1002', '1624', '1261', '1679', '1530', '1586', '1202', '1347', '160', '1844', '115', '1146', '1384', '1317', '1124', '1511', '1481', '1217', '1229', '145', '1249', '1596', '1913', '159', '1342', '1716', '1806', '1783', '1368', '1646', '1255', '1704', '1948', '1563', '2114', '1817', '1975', '1859', '1739', '1661', '1754', '1890', '1652', '1828', '1614', '1771', '1020', '1880', '1103', '1575', '1008', '1118', '1061', '1505', '1414', '1905', '1090', '2206', '1898', '1192', '1051', '2126', '1308', '1620', '1932', '1077', '2237', '1556', '1041', '177', '1007', '1960', '2188', '48', '1174', '2180', '1004', '1006', '1943', '1003', '1005', '205', '181', '153', '2760', '1422', '1424', '1946', '2233', '2164', '1187', '1001', '194', '2155', '152', '172', '1216', '154', '97', '1553', '2759', '2235', '2234', '2193', '1941', '2121', '2238', '9', '193', '1463', '1475', '2492', '2134', '1985', '2236', '2185', '2216', '180', '1471', '2212', '2147', '176', '1982', '2220', '185', '1500', '2160', '2123', '187', '2209', '2204', '2198', '2200', '2224', '174', '190', '1169', '2152', '2173', '2169', '1434', '188', '50', '198', '74', '28', '171', '2842', '150', '2796', '27', '2758', '197', '166', '208', '2795', '2797', '2845', '85', '3048', '2844', '164', '182', '199', '2855', '236', '169', '2437', '2653', '5968', '170', '62', '2510', '2843', '2858', '2778', '2793', '183', '2847', '2792', '2846', '2856', '13', '5046', '2951', '2850', '2851', '2848', '161', '192', '2950', '2794', '3668', '3034', '2854', '18', '300', '37', '157', '3663', '2849', '65', '2226', '4304', '21', '86', '200', '2364', '151', '184', '195', '2755', '93', '146', '2728', '2852', '111', '149', '2112', '2814', '158', '162', '173', '189', '5056', '2853', '2857', '2784', '7', '94', '101', '114', '210', '234', '318', '2963', '155', '163', '167', '175', '179', '196', '2729', '2952', '4491', '4637', '5051', '2790', '2890', '100', '209', '235', '305', '2105', '2227', '2357', '2362', '2494', '2495', '2498', '2807', '2815', '165', '186', '3074', '5042', '5117', '5129', '2779', '2781', '2782', '2787', '2789', '2914', '2918', '33', '38', '45', '108', '202', '203', '204', '243', '245', '303', '2110', '2231', '2232', '2353', '2361', '2396', '2730']
    stri = ['россия', 'москва', 'санкт-петербург', 'спб', 'московская область', 'беларусь', 'украина', 'казахстан', 'краснодарский край', 'минск', 'республика татарстан', 'свердловская область', 'нижегородская область', 'ростовская область', 'самарская область', 'новосибирская область', 'республика башкортостан', 'алматы', 'воронежская область', 'киев', 'красноярский край', 'челябинская область', 'пермский край', 'иркутская область', 'волгоградская область', 'ставропольский край', 'алтайский край', 'кемеровская область', 'ленинградская область', 'омская область', 'саратовская область', 'тульская область', 'астана', 'тюменская область', 'владимирская область', 'ярославская область', 'тверская область', 'хантымансийский ао  югра', 'удмуртская республика', 'томская область', 'рязанская область', 'приморский край', 'оренбургская область', 'республика крым', 'белгородская область', 'хабаровский край', 'калужская область', 'вологодская область', 'кировская область', 'ивановская область', 'липецкая область', 'чувашская республика', 'брянская область', 'ульяновская область', 'костромская область', 'калининградская область', 'курская область', 'смоленская область', 'пензенская область', 'архангельская область', 'республика бурятия', 'мурманская область', 'астраханская область', 'ямалоненецкий ао', 'тамбовская область', 'псковская область', 'харьковская область', 'орловская область', 'забайкальский край', 'новгородская область', 'днепропетровская область', 'курганская область', 'республика марий эл', 'амурская область', 'республика карелия', 'минская область', 'республика мордовия', 'республика коми', 'караганда', 'брест', 'сахалинская область', 'одесская область', 'кыргызстан', 'республика саха якутия', 'львовская область', 'могилев', 'гродно', 'камчатский край', 'гомель', 'витебск', 'шымкент', 'павлодар', 'атырау', 'бишкек', 'республика адыгея', 'республика дагестан', 'магаданская область', 'брестская область', 'киевская область', 'республика хакасия', 'другие страны', 'устькаменогорск', 'запорожская область', 'актау', 'костанай', 'республика алтай', 'актобе', 'узбекистан', 'республика калмыкия', 'ташкент', 'гомельская область', 'витебская область', 'полтавская область', 'еврейская ао', 'винницкая область', 'могилевская область', 'азербайджан', 'уральск', 'кабардинобалкарская республика', 'республика северная осетияалания', 'баку', 'донецкая область', 'ненецкий ао', 'гродненская область', 'николаевская область', 'черкасская область', 'петропавловск', 'карачаевочеркесская республика', 'хмельницкая область', 'житомирская область', 'кокшетау', 'чукотский ао', 'черниговская область', 'семей', 'чеченская республика', 'иванофранковская область', 'волынская область', 'тараз', 'херсонская область', 'тернопольская область', 'ровенская область', 'сумская область', 'черновицкая область', 'кызылорда', 'темиртау', 'республика тыва', 'закарпатская область', 'луганская область', 'кировоградская область', 'республика ингушетия', 'талдыкорган', 'китай', 'экибастуз', 'польша', 'грузия', 'капчагай', 'балыкчы', 'аксай казахстан', 'ош', 'германия', 'тбилиси', 'щучинск', 'жезказган', 'оаэ', 'джалалабад', 'баткен', 'карабалта', 'сша', 'талгар', 'кант', 'балхаш', 'рудный', 'чехия', 'токмок', 'кипр', 'зыряновск', 'степногорск', 'каскелен', 'ашукино', 'иссык', 'молдавия', 'жанаозен', 'исфана', 'чолпоната', 'самарканд', 'талас', 'риддер вко', 'карасуу', 'каракол', 'каракуль', 'токтогул', 'армения', 'черногория', 'сарыагаш', 'кызылкия', 'майлуусуу', 'кербен', 'аркалык', 'туркестан', 'макинск', 'нарын', 'кадамжай', 'кульсары', 'ташкумыр', 'бельгия', 'таиланд', 'испания', 'аягоз вко', 'атбасар', 'кочкората', 'нидерланды', 'байконур кызылорд обл', 'сарканд', 'великобритания', 'таджикистан', 'болгария', 'оман', 'аксу павлодаробл', 'сатпаев', 'хромтау', 'серебрянск', 'туркменистан', 'сербия', 'акколь', 'ноокат', 'япония', 'швеция', 'абхазия', 'батуми', 'аральск', 'баутино', 'кентау юко', 'текели', 'новоишимское', 'сулюкта', 'узген', 'коканд', 'австрия', 'турция', 'франция', 'венгрия', 'катар', 'румыния', 'словакия', 'хырдалан', 'арысь юко', 'бейнеу мангистауская обл', 'жанатас', 'казалы кызылорд обл', 'ленгер юко', 'шемонаиха вко', 'шу', 'жетысай', 'кордай', 'чунджа', 'курчатов казахстан', 'навои', 'кувасай', 'финляндия', 'индия', 'аргентина', 'египет', 'словения', 'иран', 'албания', 'королевство саудовская аравия', 'гянджа', 'сумгаит', 'ленкорань', 'хачмаз', 'рустави', 'ерейментау акмолобл', 'солнечный пгт павлодарская обл', 'кандыагаш', 'тайынша', 'осакаровка', 'бурабай', 'наманган', 'бухара', 'фергана', 'чирчик', 'термез', 'тойтепа', 'учкудук', 'израиль', 'италия', 'канада', 'швейцария', 'гвинея', 'вьетнам', 'новая зеландия', 'бразилия', 'камбоджа', 'бангладеш', 'афганистан', 'монголия', 'ирак', 'мальта', 'бахрейн', 'чили', 'доминиканская республика']

    gorod = process.extractOne(gorod, stri)
    gorod = re.sub("[0-9]", "", str(gorod))
    gorod_area = ''
    for i in range(len(stri)):
        if stri[i] == gorod[2:-4]:
            gorod_area = numbe[i]
    
    url = f"https://perm.hh.ru/search/resume?text={vakans}&area={gorod_area}&ored_clusters=true&order_by=relevance&search_period=0&logic=normal&pos=full_text&exp_period=all_time"

    def podkl(): #Подключение к сайту, получение супа
        ua =fake_useragent.UserAgent()
        headers = {'user-agent':ua.random }
        response = requests.get(url=url, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'lxml')
        return soup

    soup = podkl()
    pagen.append([link.text for link in soup.find('div', class_='pager').find_all('span') if link.text.isdigit()][-1])
    
    if int(pagen[0])> 15:
        pagen[0] = 15

    for i in range(pagen[0]):
            url_pars.append(f"{url}{'&page='}{i}")

    data = {}

    async def poluch_ssulka(url):
            while True:
                global b 
                ua = fake_useragent.UserAgent()
                fake_ua = {'user-agent': ua.random}    
                async with aiohttp.ClientSession(headers=fake_ua) as session:
                    async with session.get(url) as response:
                        soup = BeautifulSoup(await response.text(), 'lxml') 
                try:
                    ssulka_na_resume.extend([f"{'https://hh.ru/'}{link['href']}" for link in soup.find('main', class_='resume-serp-content').find_all( 'a') ]) 
                    break           
                except:
                    b = ''
                    
    async def run_tasks():
        tasks = [poluch_ssulka(link) for link in url_pars]
        await asyncio.gather(*tasks)
    
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(run_tasks())

    async def poluch_dannye(url):
            while True:           
                global a , b 
                ua = fake_useragent.UserAgent()
                fake_ua = {'user-agent': ua.random}    
                async with aiohttp.ClientSession(headers=fake_ua) as session:
                    async with session.get(url) as response:
                        soup = BeautifulSoup(await response.text(), 'lxml') 
                try:                
                    professi = ([link.text for link in soup.find('h2', class_='bloko-header-2')])
                    zarplat = ([link.text for link in soup.find_all('span', class_='resume-block__salary')])
                    grafik_rabot = ([link.text for link in soup.find('div', class_='resume-block-item-gap').find_all('p')])
                    opyt_rabot = ([link.text for link in soup.find('div', class_='bloko-gap bloko-gap_top').find('span', class_='resume-block__title-text resume-block__title-text_sub')])
                    spesialyzasy = ([link.text for link in soup.find('div', class_='resume-block-item-gap').find_all('li')])
                    rezum = ([link.text for link in soup.find('div', class_='bloko-gap bloko-gap_top').find('div', class_='bloko-tag-list')])
                    vozras = ([link.text for link in soup.find('div', class_='resume-header-title').find_all('span', attrs={'data-qa' : 'resume-personal-age'})])
                    obrazovani = ([link.text for link in soup.find('div', class_='resume-wrapper').find_all('div', attrs={'data-qa' : 'resume-block-education-organization'})])          
                    data[url] = []
                    data[url].append({
                        'profesii': professi,
                        'zarplat' : zarplat,
                        'grafic_rabot': grafik_rabot,
                        'spesialyzasy' : spesialyzasy,
                        'rezum' : rezum,
                        'vozras' : vozras,
                        'obrazovani' : obrazovani,
                        'opyt_rabot' : opyt_rabot
                    })
                    with open('data.json', 'w', encoding='utf-8') as outfile:
                        json.dump(data, outfile, ensure_ascii=False)
                    break           
                except:
                    b = ''

    async def run_r2():
        tasks = [poluch_dannye(link) for link in ssulka_na_resume]
        await asyncio.gather(*tasks)

    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(run_r2())
    print('end pars')